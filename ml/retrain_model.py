import pandas as pd
import joblib
import os
import json
import time
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    precision_score,
    recall_score,
    accuracy_score
)
from sklearn.utils import resample
from joblib import load
import traceback
from utils.preprocess import preprocess_dataset  # ‚úÖ NEW import

print("üîÑ Retraining Model from Feedback (Integrated Data Mode)...")

# --------------------------
# üîπ Paths
# --------------------------
FEEDBACK_PATH = "/ui/feedback.json"
MODEL_PATH = "/app/model.pkl"
TRAINING_LOGS = "/shared/training_logs.json"
CONFIG_PATH = "/ml/config.json"
FEATURE_IMPORTANCE_PATH = "/shared/feature_importance.json"
YTEST_OUTPUT_PATH = "/shared/y_true_pred.json"
EXPLANATION_MAP_PATH = "/shared/explanation_map.json"
DATA_DIR = "/ml"

# --------------------------
# üîπ Load model (if exists)
# --------------------------
def safe_load_model(path):
    try:
        return joblib.load(path)
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load model from {path}: {e}")
        traceback.print_exc()
        return None

model = safe_load_model(MODEL_PATH)

# --------------------------
# üîπ Load anomaly config
# --------------------------
DEFAULT_CONFIG = {"quantity_threshold": 1.0, "price_threshold": 0.01, "balance_threshold": 1.0}
if os.path.exists(CONFIG_PATH):
    try:
        with open(CONFIG_PATH, "r") as f:
            config = json.load(f)
        print(f"üß™ Loaded anomaly thresholds: {config}")
    except:
        config = {"anomaly_rules": DEFAULT_CONFIG}
        print("‚ö†Ô∏è Failed to load config. Using defaults.")
else:
    config = {"anomaly_rules": DEFAULT_CONFIG}
    print("‚ö†Ô∏è Config file not found. Using defaults.")

# --------------------------
# üîπ Load data safely
# --------------------------
def safe_read_csv(path):
    try:
        df = pd.read_csv(path)
        df.columns = df.columns.str.strip()
        return df
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not read {path}: {e}")
        return pd.DataFrame()

# --------------------------
# üîπ Process all CSVs dynamically from /ml
# --------------------------
dataframes = []
raw_anomalies = []  # ‚úÖ NEW for explanation map
for fname in os.listdir(DATA_DIR):
    if fname.endswith(".csv"):
        path = os.path.join(DATA_DIR, fname)
        df_raw = safe_read_csv(path)
        if not df_raw.empty:
            processed = preprocess_dataset(df_raw, fname, config)
            if not processed.empty:
                dataframes.append(processed)
                if "label" in df_raw.columns and "COMMENT" in df_raw.columns:
                    raw_anomalies.extend(
                        df_raw[df_raw["label"] == 1].to_dict(orient="records")
                    )

# --------------------------
# üîπ Load & process feedback.json
# --------------------------
if os.path.exists(FEEDBACK_PATH):
    try:
        feedback = json.load(open(FEEDBACK_PATH))
        df_fb = pd.DataFrame(feedback)
        fb_data = pd.json_normalize(df_fb["input"])
        fb_data["label"] = df_fb["feedback"].apply(lambda x: 1 if x == "Yes" else 0)
        fb_data = fb_data.select_dtypes(include=["number"])
        fb_data["label"] = fb_data["label"].astype(int)
        if not fb_data.empty:
            dataframes.append(fb_data)
    except Exception as e:
        print(f"‚ö†Ô∏è Feedback parse error: {e}")

# --------------------------
# üîπ Merge data for training
# --------------------------
if not dataframes:
    print("‚ùå No valid data found. Cannot train model.")
    exit()

full_data = pd.concat(dataframes, ignore_index=True)
X = full_data.drop("label", axis=1).select_dtypes(include=["number"])
y = full_data["label"]

# --------------------------
# üîπ Balance data
# --------------------------
normal_df = full_data[full_data["label"] == 0]
anomaly_df = full_data[full_data["label"] == 1]
if len(anomaly_df) < len(normal_df):
    anomaly_df = resample(anomaly_df, replace=True, n_samples=len(normal_df), random_state=42)

train_data = pd.concat([normal_df, anomaly_df]).sample(frac=1, random_state=42).reset_index(drop=True)
X_resampled = train_data.drop("label", axis=1).select_dtypes(include=["number"])
y_resampled = train_data["label"]

# --------------------------
# üîπ Train/test split & training
# --------------------------
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

if model:
    print("üîÅ Incrementally Training the Existing Model...")
    model.fit(X_train, y_train)
else:
    print("üìå Training a New Model from Scratch...")
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight={0: 1, 1: 3},
        random_state=42
    )
    model.fit(X_train, y_train)

# --------------------------
# üîπ Evaluation
# --------------------------
print("üìä Model Performance After Retraining:")
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

with open(YTEST_OUTPUT_PATH, "w") as f:
    json.dump({"y_test": y_test.tolist(), "y_pred": y_pred.tolist()}, f, indent=2)
print("üìâ Saved y_test and y_pred to /shared/y_true_pred.json")

# --------------------------
# üîπ Save model
# --------------------------
joblib.dump(model, MODEL_PATH)
print("‚úÖ Model retrained and saved to", MODEL_PATH)

# --------------------------
# üîπ Save training logs
# --------------------------
entry = {
    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    "accuracy": round(accuracy_score(y_test, y_pred), 3),
    "precision_0": round(precision_score(y_test, y_pred, pos_label=0), 3),
    "recall_0": round(recall_score(y_test, y_pred, pos_label=0), 3),
    "precision_1": round(precision_score(y_test, y_pred, pos_label=1), 3),
    "recall_1": round(recall_score(y_test, y_pred, pos_label=1), 3)
}
logs = json.load(open(TRAINING_LOGS, "r")) if os.path.exists(TRAINING_LOGS) else []
logs.append(entry)
with open(TRAINING_LOGS, "w") as f:
    json.dump(logs, f, indent=2)
print("üìú Logged training results to", TRAINING_LOGS)

# --------------------------
# üîπ Save feature importance
# --------------------------
feature_importance = dict(zip(X_resampled.columns, model.feature_importances_))
with open(FEATURE_IMPORTANCE_PATH, "w") as f:
    json.dump(feature_importance, f, indent=2)
print(f"üìä Saved feature importances to {FEATURE_IMPORTANCE_PATH}")

# --------------------------
# üîπ Save feature list
# --------------------------
with open("/shared/feature_list.json", "w") as f:
    json.dump(list(X_resampled.columns), f)
print("üßæ Saved feature list to /shared/feature_list.json")

# --------------------------
# üîπ Generate explanation_map from COMMENT field
# --------------------------
try:
    explanation_map = {}
    for row in raw_anomalies:
        numeric_key = tuple(round(float(row[k]), 3) for k in row if isinstance(row[k], (int, float)))
        comment = row.get("COMMENT") or row.get("comment") or "No COMMENT"
        explanation_map[str(numeric_key)] = str(comment)
    with open(EXPLANATION_MAP_PATH, "w") as f:
        json.dump(explanation_map, f, indent=2)
    print("üóÇÔ∏è Saved explanation_map.json from COMMENT field.")
except Exception as e:
    print("‚ö†Ô∏è Failed to generate explanation map:", e)
